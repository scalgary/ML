---
title: "Chapitre 8"
format:
  html:
    code-fold: true
jupyter: python3
---


```{python}
import numpy as np
import pandas as pd
from matplotlib.pyplot import subplots
import sklearn.model_selection as skm

```


```{python}
from sklearn.tree import (DecisionTreeClassifier ,
                          DecisionTreeRegressor ,
                          plot_tree,
                          export_text)
from sklearn.metrics import (accuracy_score,
                             log_loss)
from sklearn.ensemble import \
     (RandomForestRegressor ,
      GradientBoostingRegressor )

```


```{python}
Carseats = pd.read_csv('data/Carseats.csv')
High = np.where(Carseats.Sales > 8,
                "Yes",
                "No")

from core.helpers import transform_df_for_model
feature_names = Carseats.columns.drop('Sales')
D = transform_df_for_model(Carseats, Carseats.columns.drop('Sales'), add_intercept=False)
D = D.drop('intercept', axis=1)
feature_names = D.columns
X = np.asarray(D)


```


```{python}
clf = DecisionTreeClassifier(criterion='entropy',
          max_depth=3,
          random_state=0)        
clf.fit(X, High)

accuracy_score(High, clf.predict(X))
```


```{python}
resid_dev = np.sum(log_loss(High, clf.predict_proba(X)))
resid_dev
```


```{python}
ax = subplots(figsize=(12,12))[1]
plot_tree(clf,
          feature_names=feature_names,
          ax=ax)
```


```{python}
print(export_text(clf,
                  feature_names=feature_names,
                  show_weights=True))
```


```{python}
validation = skm.ShuffleSplit(n_splits=1,
                              test_size=200,
                              random_state=0)
results = skm.cross_validate(clf,
                             D,
                             High,
                             cv=validation)
results['test_score']
```

```{python}
X
```

```{python}
(X_train,
 X_test,
 High_train,
 High_test) = skm.train_test_split(X,
                                   High,
                                   test_size=0.5,
                                   random_state=0)


```

```{python}
clf = DecisionTreeClassifier(criterion='entropy', random_state=0)
clf.fit(X_train, High_train)
accuracy_score(High_test, clf.predict(X_test))
```
```{python}
ccp_path = clf.cost_complexity_pruning_path(X_train, High_train)
kfold = skm.KFold(10,
                  random_state=1,
                  shuffle=True)


```

```{python}
grid = skm.GridSearchCV(clf,
                        {'ccp_alpha': ccp_path.ccp_alphas},
                        refit=True,
                        cv=kfold,
                        scoring='accuracy')
grid.fit(X_train, High_train)
grid.best_score_
```
```{python}
ax = subplots(figsize=(12, 12))[1]
best_ = grid.best_estimator_
plot_tree(best_,
          feature_names=feature_names,
          ax=ax)


```


```{python}
best_.tree_.n_leaves
```

```{python}
from sklearn.metrics import confusion_matrix as _confusion_matrix
from sklearn.metrics._classification import unique_labels

```

```{python}
from core.helpers import confusion_table
print(accuracy_score(High_test,
                     best_.predict(X_test)))
confusion = confusion_table(best_.predict(X_test),
                            High_test)
confusion
```


```{python}

Boston = pd.read_csv('data/Boston.csv')
D = transform_df_for_model(Boston, Boston.columns.drop('medv'), add_intercept=False)
feature_names = list(D.columns)
X = np.asarray(D)
feature_names
```

```{python}
D
```
```{python}
(X_train,
 X_test,
 y_train,
 y_test) = skm.train_test_split(X,
                                Boston['medv'],
                                test_size=0.3,
                                random_state=0)
```


```{python}
reg = DecisionTreeRegressor(max_depth=3)
reg.fit(X_train, y_train)
ax = subplots(figsize=(12,12))[1]
plot_tree(reg,
          feature_names=feature_names,
          ax=ax)
```


```{python}
ccp_path = reg.cost_complexity_pruning_path(X_train, y_train)
kfold = skm.KFold(5,
                  shuffle=True,
                  random_state=10)
grid = skm.GridSearchCV(reg,
                        {'ccp_alpha': ccp_path.ccp_alphas},
                        refit=True,
                        cv=kfold,
                        scoring='neg_mean_squared_error')
G = grid.fit(X_train, y_train)
```


```{python}
best_ = grid.best_estimator_
np.mean((y_test - best_.predict(X_test))**2)
```


```{python}
ax = subplots(figsize=(12,12))[1]
plot_tree(G.best_estimator_,
          feature_names=feature_names,
          ax=ax)
```


```{python}
bag_boston = RandomForestRegressor(max_features=X_train.shape[1], random_state=0)
bag_boston.fit(X_train, y_train)
```


```{python}
ax = subplots(figsize=(8,8))[1]
y_hat_bag = bag_boston.predict(X_test)
ax.scatter(y_hat_bag, y_test)
np.mean((y_test - y_hat_bag)**2)
```


```{python}
bag_boston = RandomForestRegressor(max_features=X_train.shape[1],
                n_estimators=500,
                random_state=0).fit(X_train, y_train)
y_hat_bag = bag_boston.predict(X_test)
np.mean((y_test - y_hat_bag)**2)
```


```{python}
X_train
```


```{python}


def transform_df_for_model(df, terms, add_intercept=True, interactions=None, contrast ="drop"):
    import pandas as pd
    import numpy as np
    import formulaic
    from pandas.api.types import is_numeric_dtype, is_categorical_dtype
    """
    Transforms a DataFrame into a design matrix for modeling using Formulaic,
    supporting both "drop" (one-hot encoding) and "sum" (contrast sum coding).

    Features:
    - Detects categorical variables and encodes them using "drop" or "sum" encoding.
    - Ensures the final column order matches `variables`, replacing categorical variables in the correct position.
    - Supports interaction terms while keeping order.
    - Renames categorical variables in the format `oldvariable[value]`.

    Parameters:
     df : pd.DataFrame
        The DataFrame containing the dataset.
    variables : list
        A list of column names to include in the design matrix (order is preserved).
    interactions : list of tuples, optional
        A list of tuples specifying interaction terms (e.g., [('lstat', 'rm')] for `lstat * rm`).
    contrast : str, default="drop"
        - `"drop"`: Uses one-hot encoding, dropping the first observed category.
        - `"sum"`: Uses contrast sum coding (deviation coding).


    Returns:
    X : pd.DataFrame
        The design matrix with categorical variables encoded, formatted, and ordered correctly.
    """

    # 1Ô∏è‚É£ Ensure `df` is a pandas DataFrame
    if not isinstance(df, pd.DataFrame):
        raise TypeError("‚ùå Error: `df` must be a pandas DataFrame.")

    # 2Ô∏è‚É£ Ensure all specified variables exist in `df`
    missing_vars = [var for var in terms if var not in df.columns]
    if missing_vars:
        raise ValueError(f"‚ùå Error: The following columns are missing from the DataFrame: {missing_vars}")

    # 3Ô∏è‚É£ Identify categorical and numeric variables
    categorical_vars = [var for var in terms if df[var].dtype.name == "category" or df[var].dtype == "object"]
    numeric_vars = [var for var in terms if pd.api.types.infer_dtype(df[var]) in ['integer', 'floating']]

    # 4Ô∏è‚É£ Determine the first observed category for each categorical variable
    first_seen_categories = {cat_var: df[cat_var].iloc[0] for cat_var in categorical_vars}
    first_alpha_categories = {cat_var: np.sort(df[cat_var].unique())[0] for cat_var in categorical_vars}

    # 5Ô∏è‚É£ Build the formula dynamically, ensuring the first observed category is dropped
    formula_parts = []
    categorical_mapping = {}  # Track categorical variable replacements for ordering
    for var in terms:
        if var in numeric_vars:
            formula_parts.append(var)
        if var in categorical_vars:
            if contrast == "drop":
                first_category = first_alpha_categories[var]  # Drop first alphabetic category
                formula_parts.append(f"C({var}, Treatment('{first_category}'))")
            elif contrast == "sum":
                formula_parts.append(f"C({var}, Sum)")
            else:
                raise ValueError("‚ùå Error: `contrast` must be 'drop' or 'sum'.")
            


    # 6Ô∏è‚É£ Add interaction terms if provided
    if interactions:
        interaction_terms = []
        for term1, term2 in interactions:
            interaction_terms.append(f"{term1}:{term2}")  # Formulaic uses ":" for interactions
        formula_parts.extend(interaction_terms)

    # 7Ô∏è‚É£ Construct the final formula
    formula =  " + ".join(map(str, formula_parts))

    # 8Ô∏è‚É£ Generate the design matrix using Formulaic
    X = formulaic.model_matrix(formula, df)

    if not add_intercept:
        X = X.drop(['Intercept'], axis=1)

    # 9Ô∏è‚É£ Rename "Intercept" column to "intercept"
    if "Intercept" in X.columns:
        X = X.rename(columns={"Intercept": "intercept"})

    # üîü Rename categorical variable names to "oldvariable[value]" format
    new_col_names = {}
    if contrast == "drop":
        for col in X.columns:
            if "C(" in col and "Treatment" in col:  # Formulaic encodes as "C(variable, Treatment)[T.value]"
                original_var = col.split(",")[0].replace("C(", "").strip()
                category_value = col.split("[T.")[1].replace("]", "").strip()
                new_col_name = f"{original_var.strip()}[{category_value.strip()}]"
                new_col_names[col] = new_col_name
                categorical_mapping.setdefault(original_var.strip(), []).append(new_col_name)  # Track dummy variable names
    elif contrast == "sum":
        for col in X.columns:
            if "C(" in col :  # Formulaic encodes as "C(variable, Treatment)[T.value]"
                original_var = col.split(",")[0].replace("C(", "").strip()
                category_value = col.split("[T.")[1].replace("]", "").strip()
                new_col_name = f"{original_var.strip()}[{category_value.strip()}]"
                new_col_names[col] = new_col_name
                categorical_mapping.setdefault(original_var.strip(), []).append(new_col_name)  # Track dummy variable names
    # Apply renaming
    X = X.rename(columns=new_col_names)

    # üîπ 11. Reorder columns to match `variables` order exactly
    ordered_columns = ["intercept"] if "intercept" in X.columns else []

    for var in terms:
        if var in numeric_vars:
            ordered_columns.append(var)  # Numeric variables remain as is
        elif var in categorical_mapping:
            ordered_columns.extend(categorical_mapping[var])  # Replace categorical column with its dummy variables

    # Add interaction terms at the end
    if interactions:
        for term1, term2 in interactions:
            interaction_col1 = f"{term1}:{term2}"
            interaction_col2 = f"{term2}:{term1}"  # Some formats might flip order
            if interaction_col1 in X.columns:
                ordered_columns.append(interaction_col1)
            elif interaction_col2 in X.columns:
                ordered_columns.append(interaction_col2)

 

    return X

```


```{python}
Boston = pd.read_csv('data/Boston.csv')
transform_df_for_model(Boston, Boston.columns.drop('medv'), add_intercept=False)
```